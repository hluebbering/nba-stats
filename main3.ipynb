{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use Python and the NBA API to develop advanced machine learning model that predicts player performance metrics in upcoming game\n",
    "\n",
    "\n",
    "<h3 style=\"color:black;font-family:'Segoe UI Variable Display';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;font-weight:300;line-height:1;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Notes from Paris';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;line-height:1;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Juicy Advice Outline';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Mencken Std';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Digital-7';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Proxima Nova';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Barlow Condensed';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "\n",
    "\n",
    "<h3 style=\"color:black;font-family:'Lazy Crunch';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Abril Display';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;\">Part 2.0</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fetching Data ===\n",
      "  ✅ Processed data for Nikola Jokić\n",
      "  ✅ Processed data for Shai Gilgeous-Alexander\n",
      "  ✅ Processed data for Anthony Edwards\n",
      "  ✅ Processed data for Pascal Siakam\n",
      "  ✅ Processed data for Jalen Williams\n",
      "=== Applying Aggregators ===\n",
      "Rows right after per-player processing : 372\n",
      "Rows after position agg                : 0\n",
      "Rows after team-vs-opp agg             : 0\n",
      "\n",
      "=== Training Model ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 25)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#all_player_data = add_opponent_position_allowed_pts(all_player_data)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#all_player_data = add_team_vs_opponent_allowed_pts(all_player_data)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# --- Train & Evaluate Model ---\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m X_train_scaled, X_test_scaled, y_train, y_test, X_test_original \u001b[38;5;241m=\u001b[39m prepare_data(all_player_data)\n\u001b[0;32m     79\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n\u001b[0;32m     80\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m evaluate_model(best_model, X_test_scaled, y_test, X_test_original)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\OneDrive\\Desktop\\hluebbering.github.io\\nba-stats\\src\\model_training.py:94\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(df, feature_cols)\u001b[0m\n\u001b[0;32m     91\u001b[0m pid_test  \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPLAYER_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 94\u001b[0m X_train_s \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     95\u001b[0m X_test_s  \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     97\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 25)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import pandas as pd\n",
    "\n",
    "from src.data_ingestion import (\n",
    "    fetch_bulk_player_game_logs,\n",
    "    get_player_game_logs,\n",
    "    get_player_advanced_stats_parallel,\n",
    "    get_opponent_stats      # ← add this line\n",
    ")\n",
    "from src.feature_engineering import feature_engineering_pipeline\n",
    "from src.model_training import prepare_data, train_models, evaluate_model, DEFAULT_FEATURE_COLS\n",
    "from src.utils import get_player_id, get_team_abbreviation_id_mapping\n",
    "from src.aggregators import (\n",
    "    add_opponent_position_allowed_pts,\n",
    "    add_team_vs_opponent_allowed_pts\n",
    ")\n",
    "from src.data_ingestion import get_opponent_stats_last10\n",
    "from src.aggregators import compute_position_allowed_pts, add_player_position\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "PLAYER_NAMES = [\n",
    "    \"Nikola Jokić\", \"Shai Gilgeous-Alexander\", \"Anthony Edwards\", \"Tyrese Haliburton\",\n",
    "    \"Pascal Siakam\", \"Jalen Williams\", \"Chet Holmgren\"\n",
    "]\n",
    "season = '2024-25'\n",
    "\n",
    "# --- Fetch & Process Data ---\n",
    "print(\"=== Fetching Data ===\")\n",
    "bulk_logs_df = fetch_bulk_player_game_logs(season)\n",
    "team_map = get_team_abbreviation_id_mapping()\n",
    "opp_df = get_opponent_stats(season)   # ← add this line\n",
    "opp_last10_df = get_opponent_stats_last10(season)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_player_data = pd.DataFrame()\n",
    "\n",
    "for name in PLAYER_NAMES:\n",
    "    pid = get_player_id(name)\n",
    "    if not pid:\n",
    "        continue\n",
    "\n",
    "    logs = get_player_game_logs(pid, bulk_logs_df)\n",
    "    if logs.empty:\n",
    "        continue\n",
    "\n",
    "    game_ids = logs['GAME_ID'].tolist()\n",
    "    adv_stats = get_player_advanced_stats_parallel(pid, game_ids)\n",
    "    if adv_stats.empty:\n",
    "        continue\n",
    "\n",
    "    logs['PLAYER_NAME'] = name\n",
    "    merged = logs.merge(adv_stats, on=['GAME_ID', 'PLAYER_ID'], how='left')\n",
    "    merged['PLAYER_ID'] = pid\n",
    "\n",
    "    # Feature engineering\n",
    "    processed = feature_engineering_pipeline(merged, team_map=team_map,opp_df=opp_df, opp_last10_df=opp_last10_df )\n",
    "    all_player_data = pd.concat([all_player_data, processed], ignore_index=True)\n",
    "    print(f\"  ✅ Processed data for {name}\")\n",
    "\n",
    "# --- Apply Aggregator Features ---\n",
    "print(\"=== Applying Aggregators ===\")\n",
    "print(\"Rows right after per-player processing :\", len(all_player_data))\n",
    "all_player_data = add_opponent_position_allowed_pts(all_player_data)\n",
    "print(\"Rows after position agg                :\", len(all_player_data))\n",
    "all_player_data = add_team_vs_opponent_allowed_pts(all_player_data)\n",
    "print(\"Rows after team-vs-opp agg             :\", len(all_player_data))\n",
    "\n",
    "\n",
    "#all_player_data = add_opponent_position_allowed_pts(all_player_data)\n",
    "#all_player_data = add_team_vs_opponent_allowed_pts(all_player_data)\n",
    "\n",
    "# --- Train & Evaluate Model ---\n",
    "print(\"\\n=== Training Model ===\")\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, X_test_original = prepare_data(all_player_data)\n",
    "best_model = train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "eval_df = evaluate_model(best_model, X_test_scaled, y_test, X_test_original)\n",
    "\n",
    "# --- Next-Game Predictions ---\n",
    "print(\"\\n=== Predicting Next Game Points ===\")\n",
    "from src.prediction import predict_next_game\n",
    "for name in PLAYER_NAMES:\n",
    "    predict_next_game(name, DEFAULT_FEATURE_COLS, season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NBA API\n",
    "from nba_api.stats.endpoints import (\n",
    "    playergamelog, boxscoreadvancedv2,\n",
    "    leaguedashteamstats, scoreboardv2, commonplayerinfo,\n",
    "    leaguegamefinder, boxscoretraditionalv2\n",
    ")\n",
    "from nba_api.stats.static import players, teams\n",
    "\n",
    "# Scikit-Learn & Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a machine learning model to predict NBA player performance metrics like points involves several steps:\n",
    "\n",
    "Data Collection: Gather historical and current season data using the NBA API, including advanced statistics such as Player Impact Estimate (PIE), Efficiency (EFF), Player Efficiency Rating (PER), trends, opponent data, and more.\n",
    "\n",
    "Data Preprocessing: Clean and preprocess the data to prepare it for modeling.\n",
    "\n",
    "Feature Engineering: Create features that capture the important aspects influencing player performance.\n",
    "\n",
    "Model Training: Choose and train a suitable machine learning model.\n",
    "\n",
    "Model Evaluation: Assess the model's performance and fine-tune as necessary.\n",
    "\n",
    "Prediction: Use the trained model to predict future player performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
