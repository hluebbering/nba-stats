{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use Python and the NBA API to develop advanced machine learning model that predicts player performance metrics in upcoming game\n",
    "\n",
    "\n",
    "<h3 style=\"color:black;font-family:'Segoe UI Variable Display';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;font-weight:300;line-height:1;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Notes from Paris';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;line-height:1;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Juicy Advice Outline';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Mencken Std';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Digital-7';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Proxima Nova';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Barlow Condensed';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "\n",
    "\n",
    "<h3 style=\"color:black;font-family:'Lazy Crunch';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;line-height:1;margin:0;\">Part 2.0</h3>\n",
    "<h3 style=\"color:black;font-family:'Abril Display';font-size:20px;text-shadow:0.125px 0.25px 0.25px black;margin:0;\">Part 2.0</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NBA API\n",
    "from nba_api.stats.endpoints import (\n",
    "    playergamelog, boxscoreadvancedv2,\n",
    "    leaguedashteamstats, scoreboardv2, commonplayerinfo,\n",
    "    leaguegamefinder, boxscoretraditionalv2\n",
    ")\n",
    "from nba_api.stats.static import players, teams\n",
    "\n",
    "# Scikit-Learn & Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a machine learning model to predict NBA player performance metrics like points involves several steps:\n",
    "\n",
    "Data Collection: Gather historical and current season data using the NBA API, including advanced statistics such as Player Impact Estimate (PIE), Efficiency (EFF), Player Efficiency Rating (PER), trends, opponent data, and more.\n",
    "\n",
    "Data Preprocessing: Clean and preprocess the data to prepare it for modeling.\n",
    "\n",
    "Feature Engineering: Create features that capture the important aspects influencing player performance.\n",
    "\n",
    "Model Training: Choose and train a suitable machine learning model.\n",
    "\n",
    "Model Evaluation: Assess the model's performance and fine-tune as necessary.\n",
    "\n",
    "Prediction: Use the trained model to predict future player performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. Utility & Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_player_id(player_name):\n",
    "    \"\"\"Get the NBA player ID given the player's full name.\"\"\"\n",
    "    nba_players = players.get_players()\n",
    "    player = next((p for p in nba_players if p['full_name'].lower() == player_name.lower()), None)\n",
    "    return player['id'] if player else None\n",
    "\n",
    "def get_team_abbreviation_id_mapping():\n",
    "    \"\"\"Return a dict mapping team abbreviations to team IDs.\"\"\"\n",
    "    nba_teams = teams.get_teams()\n",
    "    return {team['abbreviation']: team['id'] for team in nba_teams}\n",
    "\n",
    "def get_player_team_id(player_id):\n",
    "    \"\"\"Get the player's current team ID.\"\"\"\n",
    "    try:\n",
    "        df = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        return int(df['TEAM_ID'].iloc[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_team_name(team_id):\n",
    "    \"\"\"Get the full team name given the team ID.\"\"\"\n",
    "    nba_teams = teams.get_teams()\n",
    "    team = next((t for t in nba_teams if t['id'] == team_id), None)\n",
    "    return team['full_name'] if team else 'Unknown Team'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "2. Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. Data Fetching Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_player_game_logs(player_id, season='2024-25'):\n",
    "    \"\"\"Fetch player game logs for the given season.\"\"\"\n",
    "    try:\n",
    "        gamelog = playergamelog.PlayerGameLog(player_id=player_id, season=season, timeout=60)\n",
    "        df = gamelog.get_data_frames()[0]\n",
    "        df.columns = df.columns.str.upper()\n",
    "        return df\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_player_advanced_stats(player_id, season='2024-25'):\n",
    "    \"\"\"Fetch advanced stats for all games played by the player in the specified season.\"\"\"\n",
    "    gamelog_df = get_player_game_logs(player_id, season)\n",
    "    adv_stats = []\n",
    "    for game_id in gamelog_df['GAME_ID']:\n",
    "        try:\n",
    "            boxscore = boxscoreadvancedv2.BoxScoreAdvancedV2(game_id=game_id, timeout=60)\n",
    "            p_stats = boxscore.player_stats.get_data_frame()\n",
    "            p_adv = p_stats[p_stats['PLAYER_ID'] == int(player_id)]\n",
    "            adv_stats.append(p_adv)\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if adv_stats:\n",
    "        df = pd.concat(adv_stats, ignore_index=True)\n",
    "        return df[['GAME_ID', 'PLAYER_ID', 'USG_PCT', 'PIE', 'TEAM_ID', 'OFF_RATING', 'PACE_PER40']]\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_opponent_stats(season='2024-25'):\n",
    "    \"\"\"Fetch opponent defensive stats for all teams.\"\"\"\n",
    "    df = leaguedashteamstats.LeagueDashTeamStats(\n",
    "        season=season, measure_type_detailed_defense='Defense',\n",
    "        per_mode_detailed='PerGame', timeout=60\n",
    "    ).get_data_frames()[0]\n",
    "    return df[['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "3. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "def compute_efficiency(df):\n",
    "    \"\"\"Compute efficiency metric = (PTS + REB + AST + STL + BLK) - (FGA - FGM) - (FTA - FTM) - TOV.\"\"\"\n",
    "    df['EFF'] = (df['PTS'] + df['REB'] + df['AST'] + df['STL'] + df['BLK']\n",
    "                 - (df['FGA'] - df['FGM']) - (df['FTA'] - df['FTM']) - df['TOV'])\n",
    "    return df\n",
    "\n",
    "def compute_true_shooting_percentage(df):\n",
    "    df['TS_DENOM'] = 2 * (df['FGA'] + 0.44 * df['FTA'])\n",
    "    df['TS_PCT'] = df.apply(\n",
    "        lambda row: row['PTS'] / row['TS_DENOM'] if row['TS_DENOM'] != 0 else 0, axis=1\n",
    "    )\n",
    "    df.drop(columns=['TS_DENOM'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_player_position(player_id, cache=None):\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    if player_id in cache:\n",
    "        return cache[player_id]\n",
    "    pos = None\n",
    "    try:\n",
    "        info = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        raw_pos = info.get('POSITION', [''])[0]\n",
    "        if isinstance(raw_pos, str):\n",
    "            main_pos = raw_pos.split('-')[0].title()\n",
    "            if 'Guard' in main_pos:\n",
    "                pos = 'G'\n",
    "            elif 'Forward' in main_pos:\n",
    "                pos = 'F'\n",
    "            elif 'Center' in main_pos:\n",
    "                pos = 'C'\n",
    "    except:\n",
    "        pass\n",
    "    cache[player_id] = pos\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(player_df, adv_df, opp_df, team_map):\n",
    "    # 1) Basic computations\n",
    "    player_df = compute_efficiency(player_df)\n",
    "    player_df = compute_true_shooting_percentage(player_df)\n",
    "\n",
    "    # 2) Merge advanced stats\n",
    "    df = pd.merge(player_df, adv_df, on=['GAME_ID', 'PLAYER_ID'], how='left')\n",
    "    df['OPPONENT_ABBREVIATION'] = df['MATCHUP'].str.split(' ').str[-1]\n",
    "    df['OPPONENT_TEAM_ID'] = df['OPPONENT_ABBREVIATION'].map(team_map)\n",
    "\n",
    "    # 3) Merge defensive stats\n",
    "    if opp_df is not None and not opp_df.empty:\n",
    "        df = pd.merge(df, opp_df, left_on='OPPONENT_TEAM_ID', right_on='TEAM_ID', how='left')\n",
    "        for col in ['DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE']:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    # 4) Sort by date and parse\n",
    "    df.sort_values(['PLAYER_NAME', 'GAME_DATE'], inplace=True)\n",
    "    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'], errors='coerce')\n",
    "\n",
    "    # 5) Convert minutes to float\n",
    "    def parse_minutes(x):\n",
    "        if isinstance(x, str) and ':' in x:\n",
    "            mins, secs = x.split(':')\n",
    "            return float(mins) + float(secs)/60\n",
    "        return float(x) if pd.notna(x) else 0\n",
    "\n",
    "    df['MIN'] = df['MIN'].apply(parse_minutes)\n",
    "    df['FG_PCT'] = df['FGM'] / df['FGA'].replace(0, np.nan)\n",
    "\n",
    "    # 6) Compute rolling stats\n",
    "    rolling_cols = ['PIE', 'USG_PCT', 'PTS', 'REB', 'AST', 'EFF', 'TS_PCT', 'MIN', 'FG_PCT', 'OFF_RATING', 'PACE_PER40']\n",
    "    for c in rolling_cols:\n",
    "        df[f'{c}_AVG_LAST_5'] = (df.groupby('PLAYER_NAME')[c]\n",
    "                                 .transform(lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()))\n",
    "        \n",
    "    for c in ['PTS', 'USG_PCT', 'MIN']:\n",
    "        df[f'{c}_VOL_LAST_5'] = (df.groupby('PLAYER_NAME')[c]\n",
    "                                 .transform(lambda x: x.shift(1).rolling(window=5, min_periods=1).std()))\n",
    "\n",
    "    # 7) Compute season average for PTS\n",
    "    df['PTS_SEASON_AVG'] = (df.groupby('PLAYER_NAME')['PTS']\n",
    "                              .transform(lambda x: x.shift(1).expanding().mean()))\n",
    "\n",
    "    # 8) Additional features\n",
    "    df['HOME_GAME'] = df['MATCHUP'].apply(lambda x: 1 if 'vs.' in x else 0)\n",
    "    df['REST_DAYS'] = df.groupby('PLAYER_NAME')['GAME_DATE'].diff().dt.days.fillna(0)\n",
    "\n",
    "    # Rename and drop extraneous columns\n",
    "    if 'TEAM_ID_x' in df.columns:\n",
    "        df.rename(columns={'TEAM_ID_x': 'TEAM_ID'}, inplace=True)\n",
    "    df.drop(columns=['TEAM_ID_y'], errors='ignore', inplace=True)\n",
    "\n",
    "    # Forward/back fill to handle any missing rolling stats\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. Aggregator Functions (Position & Team vs Opponent)\n",
    "# =============================================================================\n",
    "\n",
    "def add_player_position(df):\n",
    "    \"\"\"Add a 'POSITION' column (G, F, C) to each row.\"\"\"\n",
    "    cache = {}\n",
    "    df['POSITION'] = df['PLAYER_ID'].apply(lambda pid: get_player_position(pid, cache))\n",
    "    return df.dropna(subset=['POSITION'])\n",
    "\n",
    "def compute_position_allowed_pts(df):\n",
    "    \"\"\"Calculate how many points each team concedes on average to a specific position.\"\"\"\n",
    "    agg = df.groupby(['OPPONENT_TEAM_ID', 'POSITION'])['PTS'].mean().reset_index()\n",
    "    agg.rename(columns={'PTS': 'OPPONENT_POSITION_ALLOWED_PTS'}, inplace=True)\n",
    "    return agg\n",
    "\n",
    "def add_opponent_position_allowed_pts(df):\n",
    "    \"\"\"Merge the 'OPPONENT_POSITION_ALLOWED_PTS' back to the main DataFrame.\"\"\"\n",
    "    df_pos = add_player_position(df)\n",
    "    agg = compute_position_allowed_pts(df_pos)\n",
    "    return pd.merge(df_pos, agg, on=['OPPONENT_TEAM_ID', 'POSITION'], how='left')\n",
    "\n",
    "def compute_team_vs_opponent_allowed_pts(df):\n",
    "    \"\"\"Compute average points a TEAM_ID scores vs. a specific OPPONENT_TEAM_ID.\"\"\"\n",
    "    agg = (df.groupby(['TEAM_ID', 'OPPONENT_TEAM_ID'])['PTS']\n",
    "             .mean().reset_index(name='TEAM_VS_OPP_ALLOWED_PTS'))\n",
    "    return agg\n",
    "\n",
    "def add_team_vs_opponent_allowed_pts(df):\n",
    "    \"\"\"Merge 'TEAM_VS_OPP_ALLOWED_PTS' back to the main DataFrame.\"\"\"\n",
    "    agg = compute_team_vs_opponent_allowed_pts(df)\n",
    "    return pd.merge(df, agg, on=['TEAM_ID', 'OPPONENT_TEAM_ID'], how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "4. Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. Data Splitting & Preparation\n",
    "# =============================================================================\n",
    "\n",
    "DEFAULT_FEATURE_COLS = [\n",
    "    'PIE_AVG_LAST_5', 'USG_PCT_AVG_LAST_5', 'EFF_AVG_LAST_5', 'TS_PCT_AVG_LAST_5',\n",
    "    'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE', 'HOME_GAME', 'REST_DAYS',\n",
    "    'PTS_AVG_LAST_5', 'REB_AVG_LAST_5', 'AST_AVG_LAST_5', 'FG_PCT_AVG_LAST_5',\n",
    "    'MIN_AVG_LAST_5', 'OFF_RATING_AVG_LAST_5', 'PACE_PER40_AVG_LAST_5', 'PTS_SEASON_AVG', \n",
    "    'OPPONENT_POSITION_ALLOWED_PTS', 'TEAM_VS_OPP_ALLOWED_PTS',\n",
    "    'PTS_VOL_LAST_5', 'USG_PCT_VOL_LAST_5', 'MIN_VOL_LAST_5',\n",
    "    #'STARTERS_MISSING'  # newly added feature\n",
    "]\n",
    "feature_columns_list = ['PIE_AVG_LAST_5', 'USG_PCT_AVG_LAST_5', 'EFF_AVG_LAST_5', 'TS_PCT_AVG_LAST_5',\n",
    "    'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE', 'HOME_GAME', 'REST_DAYS',\n",
    "    'PTS_AVG_LAST_5', 'REB_AVG_LAST_5', 'AST_AVG_LAST_5', 'FG_PCT_AVG_LAST_5',\n",
    "    'MIN_AVG_LAST_5', 'OFF_RATING_AVG_LAST_5', 'PACE_PER40_AVG_LAST_5', 'PTS_SEASON_AVG', \n",
    "    'OPPONENT_POSITION_ALLOWED_PTS', 'TEAM_VS_OPP_ALLOWED_PTS',\n",
    "    'PTS_VOL_LAST_5', 'USG_PCT_VOL_LAST_5', 'MIN_VOL_LAST_5',\n",
    "    #'STARTERS_MISSING'  # newly added feature\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, feature_cols=DEFAULT_FEATURE_COLS):\n",
    "    df = df.dropna(subset=feature_cols)\n",
    "    X = df[feature_cols].copy()\n",
    "    X['PLAYER_NAME'] = df['PLAYER_NAME'] # Keep player_name for reference\n",
    "    y = df['PTS']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    X_test_original = X_test.copy() # Keep a copy of X_test before scaling\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.drop(columns=['PLAYER_NAME']))\n",
    "    X_test_scaled = scaler.transform(X_test.drop(columns=['PLAYER_NAME']))\n",
    "\n",
    "    os.makedirs('lib', exist_ok=True)\n",
    "    joblib.dump(scaler, 'lib/scaler.pkl')\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X_test_original #X_test['PLAYER_NAME'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. Modeling & Evaluation\n",
    "# =============================================================================\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ridge': Ridge(),\n",
    "        'BayesianRidge': BayesianRidge(),\n",
    "        #'Lasso': Lasso(),\n",
    "        #'ElasticNet': ElasticNet(),\n",
    "        #'LassoLars': LassoLars(),\n",
    "        #'SGDRegressor': SGDRegressor(),\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        print(f\"\\n{name} Performance:\\n  RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "\n",
    "    print(f\"\\nBest model: {type(best_model).__name__} with RMSE: {best_rmse:.2f}\")\n",
    "    return best_model\n",
    "\n",
    "def evaluate_model(model, X_test_scaled, y_test, X_test_original):\n",
    "    preds = model.predict(X_test_scaled) # Make predictions\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds)) # Evaluate metrics\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"\\nEvaluation on Test Data:\\n  RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")\n",
    "    \n",
    "    # Build eval_df \n",
    "    eval_df = X_test_original.reset_index(drop=True).copy()\n",
    "    eval_df['Actual_PTS']    = y_test.reset_index(drop=True)\n",
    "    eval_df['Predicted_PTS'] = preds\n",
    "    eval_df['Residual'] = eval_df['Actual_PTS'] - eval_df['Predicted_PTS']\n",
    "    \n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(eval_df[['PLAYER_NAME', 'Actual_PTS', 'Predicted_PTS', 'Residual']].head(10))    \n",
    "    return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "\n",
    "7. Model Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. Next-Game Prediction Logic\n",
    "# =============================================================================\n",
    "\n",
    "def get_team_defensive_stats(team_id, season='2024-25'):\n",
    "    try:\n",
    "        df = leaguedashteamstats.LeagueDashTeamStats(\n",
    "            team_id_nullable=team_id, season=season,\n",
    "            measure_type_detailed_defense='Defense', per_mode_detailed='PerGame',\n",
    "            timeout=60\n",
    "        ).get_data_frames()[0]\n",
    "        return df[['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE']].iloc[0]\n",
    "    except:\n",
    "        return pd.Series([np.nan]*4, index=['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE'])\n",
    "\n",
    "\n",
    "def get_next_game_info(player_team_id):\n",
    "    next_game_date = datetime.now() #+ timedelta(days=1)\n",
    "    max_days_ahead = 14\n",
    "\n",
    "    for _ in range(max_days_ahead):\n",
    "        game_date_str = next_game_date.strftime('%Y-%m-%d')\n",
    "        try:\n",
    "            scoreboard = scoreboardv2.ScoreboardV2(game_date=game_date_str)\n",
    "            games = scoreboard.game_header.get_data_frame()\n",
    "            team_games = games[(games['HOME_TEAM_ID'] == player_team_id) | (games['VISITOR_TEAM_ID'] == player_team_id)]\n",
    "            if not team_games.empty:\n",
    "                next_game = team_games.iloc[0]\n",
    "                opponent_team_id = next_game['VISITOR_TEAM_ID'] if next_game['HOME_TEAM_ID'] == player_team_id else next_game['HOME_TEAM_ID']\n",
    "                home_game = 1 if next_game['HOME_TEAM_ID'] == player_team_id else 0\n",
    "                return next_game_date, opponent_team_id, home_game\n",
    "        except:\n",
    "            pass\n",
    "        next_game_date += timedelta(days=1)\n",
    "\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_for_prediction(player_id, player_name, season='2024-25', feature_cols=DEFAULT_FEATURE_COLS, \n",
    "                                    df_agg_position=None, df_agg_team_vs_opp=None):\n",
    "    # 1) Pull player's logs and advanced stats.\n",
    "    logs_df = get_player_game_logs(player_id, season)\n",
    "    if logs_df.empty:\n",
    "        print(f\"No game logs for {player_name} in {season}.\")\n",
    "        return None, None\n",
    "    logs_df['GAME_DATE'] = pd.to_datetime(logs_df['GAME_DATE'])\n",
    "    logs_df.sort_values('GAME_DATE', ascending=False, inplace=True)\n",
    "    logs_df['PLAYER_NAME'] = player_name\n",
    "\n",
    "    # Team & next game\n",
    "    p_team_id = get_player_team_id(player_id)\n",
    "    next_game_date, opp_team_id, home_game = get_next_game_info(p_team_id)\n",
    "    if not next_game_date:\n",
    "        print(f\"No upcoming game found for {player_name}.\")\n",
    "        return None, None\n",
    "\n",
    "    # Opponent stats\n",
    "    opp_stats = get_team_defensive_stats(opp_team_id, season)\n",
    "    # Advanced stats\n",
    "    adv_df = get_player_advanced_stats(player_id, season)\n",
    "\n",
    "    # Build a small historical subset\n",
    "    team_map = get_team_abbreviation_id_mapping()\n",
    "    recent_logs = logs_df[logs_df['GAME_DATE'] <= logs_df['GAME_DATE'].max()]\n",
    "    adv_df = adv_df[adv_df['GAME_ID'].isin(recent_logs['GAME_ID'])]\n",
    "    final_df = feature_engineering(recent_logs, adv_df, None, team_map)\n",
    "\n",
    "    if final_df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # 2) Feature-engineer a 'latest_data' row (from last game).\n",
    "    # 3) Insert upcoming game info: rest days, home/away, opponent stats.\n",
    "    latest_data = final_df.iloc[-1].copy()\n",
    "    latest_data['REST_DAYS'] = (next_game_date - latest_data['GAME_DATE']).days\n",
    "    latest_data['HOME_GAME'] = home_game\n",
    "    latest_data['GAME_DATE'] = next_game_date\n",
    "    latest_data['OPPONENT_TEAM_ID'] = opp_team_id\n",
    "    latest_data['DEF_RATING'] = opp_stats['DEF_RATING']\n",
    "    latest_data['OPP_PTS_OFF_TOV'] = opp_stats['OPP_PTS_OFF_TOV']\n",
    "    latest_data['OPP_PTS_2ND_CHANCE'] = opp_stats['OPP_PTS_2ND_CHANCE']\n",
    "    \n",
    "    # 4) Merge aggregator stats for position and team vs. opponent.\n",
    "    # Position-based aggregator\n",
    "    if df_agg_position is not None:\n",
    "        pos = get_player_position(player_id)\n",
    "        if not pos:  # fallback\n",
    "            pos = 'G'\n",
    "        row_match = df_agg_position[\n",
    "            (df_agg_position['OPPONENT_TEAM_ID'] == opp_team_id) & (df_agg_position['POSITION'] == pos)\n",
    "        ]\n",
    "        if not row_match.empty:\n",
    "            latest_data['OPPONENT_POSITION_ALLOWED_PTS'] = row_match['OPPONENT_POSITION_ALLOWED_PTS'].iloc[0]\n",
    "        else:\n",
    "            # fallback to position average\n",
    "            fallback_pos = df_agg_position[df_agg_position['POSITION'] == pos]\n",
    "            latest_data['OPPONENT_POSITION_ALLOWED_PTS'] = fallback_pos['OPPONENT_POSITION_ALLOWED_PTS'].mean()\n",
    "\n",
    "    # Team vs Opp aggregator\n",
    "    if df_agg_team_vs_opp is not None:\n",
    "        row_match = df_agg_team_vs_opp[\n",
    "            (df_agg_team_vs_opp['TEAM_ID'] == p_team_id) & (df_agg_team_vs_opp['OPPONENT_TEAM_ID'] == opp_team_id)\n",
    "        ]\n",
    "        if not row_match.empty:\n",
    "            latest_data['TEAM_VS_OPP_ALLOWED_PTS'] = row_match['TEAM_VS_OPP_ALLOWED_PTS'].iloc[0]\n",
    "        else:\n",
    "            latest_data['TEAM_VS_OPP_ALLOWED_PTS'] = df_agg_team_vs_opp['TEAM_VS_OPP_ALLOWED_PTS'].mean()\n",
    "\n",
    "    # Build the final feature vector\n",
    "    try:\n",
    "        fv = latest_data[feature_cols].values.reshape(1, -1)\n",
    "        return fv, latest_data\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing columns for {player_name}: {e}\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_upcoming_points(player_name, season='2024-25', feature_cols=DEFAULT_FEATURE_COLS, \n",
    "                            df_agg_position=None, df_agg_team_vs_opp=None):\n",
    "    player_id = get_player_id(player_name)\n",
    "    if not player_id:\n",
    "        print(f\"Invalid player: {player_name}\")\n",
    "        return None\n",
    "\n",
    "    fv, latest_data = prepare_features_for_prediction(player_id, player_name, season, feature_cols=feature_cols,\n",
    "                                                      df_agg_position=df_agg_position, df_agg_team_vs_opp=df_agg_team_vs_opp)\n",
    "    if fv is None:\n",
    "        return None\n",
    "    \n",
    "    # Scale & predict\n",
    "    scaler = joblib.load('lib/scaler.pkl')\n",
    "    model = joblib.load('lib/player_points_model.pkl')\n",
    "    fv_scaled = scaler.transform(fv)\n",
    "    pred = model.predict(fv_scaled)\n",
    "    opp_name = get_team_name(latest_data['OPPONENT_TEAM_ID'])\n",
    "    date_str = latest_data['GAME_DATE'].strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Predicted points for {player_name} on {date_str} vs {opp_name}: {pred[0]:.2f}\")\n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "8. Feature Importance\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m reraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m      5\u001b[0m player_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeBron James\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKevin Durant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStephen Curry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiannis Antetokounmpo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLuka Dončić\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJoel Embiid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \n\u001b[0;32m     18\u001b[0m     ]\n\u001b[0;32m     20\u001b[0m season \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-25\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 21\u001b[0m opponent_stats \u001b[38;5;241m=\u001b[39m get_opponent_stats(season)\n\u001b[0;32m     22\u001b[0m team_map \u001b[38;5;241m=\u001b[39m get_team_abbreviation_id_mapping()\n\u001b[0;32m     24\u001b[0m all_player_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mget_opponent_stats\u001b[1;34m(season)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_opponent_stats\u001b[39m(season\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-25\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch opponent defensive stats for all teams.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     df \u001b[38;5;241m=\u001b[39m leaguedashteamstats\u001b[38;5;241m.\u001b[39mLeagueDashTeamStats(\n\u001b[0;32m     37\u001b[0m         season\u001b[38;5;241m=\u001b[39mseason, measure_type_detailed_defense\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefense\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     38\u001b[0m         per_mode_detailed\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerGame\u001b[39m\u001b[38;5;124m'\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[0;32m     39\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_data_frames()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEAM_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEF_RATING\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPP_PTS_OFF_TOV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPP_PTS_2ND_CHANCE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\nba_api\\stats\\endpoints\\leaguedashteamstats.py:173\u001b[0m, in \u001b[0;36mLeagueDashTeamStats.__init__\u001b[1;34m(self, last_n_games, measure_type_detailed_defense, month, opponent_team_id, pace_adjust, per_mode_detailed, period, plus_minus, rank, season, season_type_all_star, conference_nullable, date_from_nullable, date_to_nullable, division_simple_nullable, game_scope_simple_nullable, game_segment_nullable, league_id_nullable, location_nullable, outcome_nullable, po_round_nullable, player_experience_nullable, player_position_abbreviation_nullable, season_segment_nullable, shot_clock_range_nullable, starter_bench_nullable, team_id_nullable, two_way_nullable, vs_conference_nullable, vs_division_nullable, proxy, headers, timeout, get_request)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLastNGames\u001b[39m\u001b[38;5;124m\"\u001b[39m: last_n_games,\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasureType\u001b[39m\u001b[38;5;124m\"\u001b[39m: measure_type_detailed_defense,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVsDivision\u001b[39m\u001b[38;5;124m\"\u001b[39m: vs_division_nullable,\n\u001b[0;32m    171\u001b[0m }\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_request:\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_request()\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\nba_api\\stats\\endpoints\\leaguedashteamstats.py:176\u001b[0m, in \u001b[0;36mLeagueDashTeamStats.get_request\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_request\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnba_response \u001b[38;5;241m=\u001b[39m NBAStatsHTTP()\u001b[38;5;241m.\u001b[39msend_api_request(\n\u001b[0;32m    177\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[0;32m    178\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[0;32m    179\u001b[0m         proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy,\n\u001b[0;32m    180\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    181\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_response()\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\nba_api\\library\\http.py:146\u001b[0m, in \u001b[0;36mNBAHTTP.send_api_request\u001b[1;34m(self, endpoint, parameters, referer, proxy, headers, timeout, raise_exception_on_error)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading from file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents:\n\u001b[1;32m--> 146\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    147\u001b[0m         url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[0;32m    148\u001b[0m         params\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    149\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    150\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    151\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39murl\n\u001b[0;32m    154\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\luebh\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. Example Usage \n",
    "# =============================================================================\n",
    "\n",
    "player_names = [\n",
    "    \"LeBron James\", \"Kevin Durant\", \"Stephen Curry\",\n",
    "    \"Giannis Antetokounmpo\", \"Luka Dončić\", \"Joel Embiid\",\n",
    "    \"Jayson Tatum\", \"Nikola Jokić\", \"Shai Gilgeous-Alexander\",\n",
    "    \"Karl-Anthony Towns\", \"Victor Wembanyama\", \"Damian Lillard\",\n",
    "    \"Donovan Mitchell\", \"Anthony Davis\", \"Domantas Sabonis\",\n",
    "    \"James Harden\", \"Kyrie Irving\", \"Anthony Edwards\", \"Jimmy Butler\",\n",
    "    \"De'Aaron Fox\", \"Jalen Brunson\", #\"Bronny James\",\n",
    "    \"Trae Young\", \"Pascal Siakam\", \"Jalen Green\",\n",
    "    \"Darius Garland\", \"Zion Williamson\", \"Jalen Williams\",\n",
    "    \"Jaylen Brown\",  \"Paolo Bunchero\", \"Tyrese Maxey\", \n",
    "    \"Norman Powell\", \"Alperen Şengün\", \"Ja Morant\", \"Jaren Jackson Jr.\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "season = '2024-25'\n",
    "opponent_stats = get_opponent_stats(season)\n",
    "team_map = get_team_abbreviation_id_mapping()\n",
    "\n",
    "all_player_data = pd.DataFrame()\n",
    "for p_name in player_names:\n",
    "    p_id = get_player_id(p_name)\n",
    "    if not p_id:\n",
    "        continue\n",
    "    p_gamelog = get_player_game_logs(p_id, season)\n",
    "    adv_stats = get_player_advanced_stats(p_id, season)\n",
    "    if p_gamelog.empty or adv_stats.empty:\n",
    "        continue\n",
    "    \n",
    "    p_gamelog['PLAYER_NAME'] = p_name\n",
    "    merged_df = feature_engineering(p_gamelog, adv_stats, opponent_stats, team_map)\n",
    "    all_player_data = pd.concat([all_player_data, merged_df], ignore_index=True)\n",
    "\n",
    "# 3) Position & Team aggregator\n",
    "all_player_data = add_opponent_position_allowed_pts(all_player_data)\n",
    "all_player_data = add_team_vs_opponent_allowed_pts(all_player_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled, X_test_scaled, y_train, y_test, p_names_test = prepare_data(all_player_data)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, X_test_original = prepare_data(all_player_data) \n",
    "best_model = train_and_evaluate_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "joblib.dump(best_model, 'lib/player_points_model.pkl')\n",
    "\n",
    "# Build eval_df for residual analysis | after training:\n",
    "eval_df = evaluate_model(best_model, X_test_scaled, y_test, X_test_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Predict upcoming game for each player\n",
    "#    (requires the aggregator dataframes if we want position-based features)\n",
    "df_agg_position = compute_position_allowed_pts(all_player_data)  # merges OPP_TEAM & POSITION -> mean(PTS)\n",
    "df_agg_team_opp = compute_team_vs_opponent_allowed_pts(all_player_data)\n",
    "for name in player_names:\n",
    "    predict_upcoming_points(\n",
    "        name, season, df_agg_position=df_agg_position, df_agg_team_vs_opp=df_agg_team_opp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "###### Evaluation Residuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we can do residual analysis\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(eval_df['Residual'], kde=True, bins=20)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Starters Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Time Series Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def time_series_cv_evaluation(df, feature_cols, target_col='PTS', n_splits=5):\n",
    "    \"\"\"Perform time-series cross-validation with n_splits folds.\"\"\"\n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(by='GAME_DATE').reset_index(drop=True)\n",
    "    X_full = df_sorted[feature_cols].copy()\n",
    "    y_full = df_sorted[target_col].values\n",
    "\n",
    "    # Prepare cross-validator\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "    fold_number = 1\n",
    "    for train_index, val_index in tscv.split(X_full):\n",
    "        X_train, X_val = X_full.iloc[train_index], X_full.iloc[val_index] # Split\n",
    "        y_train, y_val = y_full[train_index], y_full[val_index]\n",
    "\n",
    "        scaler = StandardScaler() # Scale\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "        # (You can choose whichever model you want here. Let's do a simple RandomForest as an example.)\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict\n",
    "        val_preds = model.predict(X_val_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "        r2 = model.score(X_val_scaled, y_val)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        print(f\"Fold {fold_number} RMSE = {rmse:.3f} R-Squared = {r2:.3f}\")\n",
    "        fold_number += 1\n",
    "\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    print(f\"\\nAverage RMSE over {n_splits} folds: {avg_rmse:.3f}\")\n",
    "    print(f\"\\nAverage R-Squared over {n_splits} folds: {avg_r2:.3f}\")\n",
    "    return avg_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cv_evaluation(all_player_data, DEFAULT_FEATURE_COLS, target_col='PTS', n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Position and Role-Based Features:\n",
    "\n",
    "Include data about the player's role (e.g., starter vs. bench), player position, and how the upcoming opponent typically defends that position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.build_player_team_data import main\n",
    "df_player_full = main(season='2024-25', data_file=\"data/merged_player_team_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hyperparameter Tuning and Bayesian Optimization:\n",
    "Instead of a simple grid search, use advanced hyperparameter optimization methods (e.g., Bayesian optimization or Optuna) to find the best parameters for CatBoost, XGBoost, LightGBM, or neural networks.\n",
    "Non-Linear Models and Neural Networks:\n",
    "Consider deep learning approaches. A simple feed-forward neural network or LSTM/RNN if you structure your data as a time series could capture temporal dependencies more effectively.\n",
    "Time-Series Aware Validation:\n",
    "\n",
    "Ensure you use proper time-series cross-validation (e.g., TimeSeriesSplit) so the model isn't accidentally leaking future information.\n",
    "\n",
    "\n",
    "Betting Lines or Market Data:\n",
    "Market-based indicators (like the Vegas over/under for the game) can indirectly capture external knowledge about expected scoring environment.\n",
    "\n",
    "\n",
    "Dimensionality Reduction and Feature Selection:\n",
    "\n",
    "Feature Importance and Pruning:\n",
    "Use model explainability tools (SHAP, feature_importances_) to identify less useful features and remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Star Player Out\n",
    "\n",
    "Handling Injuries / Roster Changes\n",
    "\n",
    "If a key teammate is absent, a star player’s usage might spike. Consider adding a feature that tracks “number of typical starters missing” for a given game. That often impacts scoring opportunities.\n",
    "\n",
    "Explainability\n",
    "\n",
    "Tools like SHAP or feature importances from tree-based models can help you see which inputs drive the predictions. This can help you debug or refine features.\n",
    "\n",
    "\n",
    "\n",
    "######  Pipeline Packaging\n",
    "\n",
    "Once stable, you can wrap the entire pipeline in a script (or notebook) that daily:\n",
    "Pulls updated logs.\n",
    "Retrains/updates model (if desired) or just re-scores with the existing model.\n",
    "Outputs next-game predictions to a CSV or database.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n",
    "# Injury Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_espn_injuries(url=\"https://www.espn.com/nba/injuries\"):\n",
    "    \"\"\"Returns a DataFrame with columns: [TEAM_NAME, PLAYER_NAME, POS, EST_RETURN, STATUS, COMMENT].\"\"\"\n",
    "    # 1) Add headers that mimic a real browser\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/107.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    # 2) Make the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 3) Parse HTML\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # 4) The main container for injuries is identified by:\n",
    "    #    <div class=\"ResponsiveTable Table__league-injuries\"> ...table content...\n",
    "    #    We'll find all such sections for different teams.\n",
    "\n",
    "    # \"ResponsiveTable Table__league-injuries\" is repeated per team\n",
    "    # We might see multiple <div> blocks with that class\n",
    "    team_tables = soup.find_all(\"div\", class_=\"ResponsiveTable Table__league-injuries\")\n",
    "\n",
    "    all_rows = [] # We'll store results in a list of dicts\n",
    "\n",
    "    for table_div in team_tables:\n",
    "        # Each 'table_div' should contain a <div class=\"Table__Title\"> for the team name\n",
    "        # and a <table> with <thead>/<tbody> for the injuries.\n",
    "\n",
    "        # 1) Get the Team Name\n",
    "        title_div = table_div.find(\"div\", class_=\"Table__Title\")\n",
    "        if not title_div:\n",
    "            # If we can't find the title, skip\n",
    "            continue\n",
    "\n",
    "        # The team name is often in: <span class=\"injuries__teamName ...\">TEAM NAME</span>\n",
    "        team_name_span = title_div.find(\"span\", class_=\"injuries__teamName\")\n",
    "        if not team_name_span:\n",
    "            continue\n",
    "        team_name = team_name_span.get_text(strip=True)\n",
    "\n",
    "        # 2) The <table> has a <thead> and <tbody> with multiple <tr> rows.\n",
    "        # Typically: <tbody class=\"Table__TBODY\"><tr> ... <td> ... etc.\n",
    "        table_tag = table_div.find(\"table\", class_=\"Table\")\n",
    "        if not table_tag:\n",
    "            continue\n",
    "\n",
    "        tbody = table_tag.find(\"tbody\", class_=\"Table__TBODY\")\n",
    "        if not tbody:\n",
    "            continue\n",
    "\n",
    "        # 3) Each row in the <tbody> is one player's injury record\n",
    "        rows = tbody.find_all(\"tr\", class_=\"Table__TR\")\n",
    "        for row in rows:\n",
    "            # We have multiple <td> columns: NAME, POS, EST. RETURN DATE, STATUS, COMMENT\n",
    "            tds = row.find_all(\"td\", class_=\"Table__TD\")\n",
    "            if len(tds) < 5:\n",
    "                # Expect at least 5 columns\n",
    "                continue\n",
    "\n",
    "            player_name = tds[0].get_text(strip=True)\n",
    "            pos = tds[1].get_text(strip=True)\n",
    "            est_return = tds[2].get_text(strip=True)\n",
    "            status = tds[3].get_text(strip=True)\n",
    "            comment = tds[4].get_text(strip=True)\n",
    "\n",
    "            # Store in a dict\n",
    "            all_rows.append({\n",
    "                \"TEAM_NAME\": team_name,\n",
    "                \"PLAYER_NAME\": player_name,\n",
    "                \"POS\": pos,\n",
    "                \"EST_RETURN\": est_return,\n",
    "                \"STATUS\": status, \n",
    "                \"COMMENT\": comment\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_injury = pd.DataFrame(all_rows)\n",
    "    return df_injury\n",
    "\n",
    "\n",
    "df_injury = scrape_espn_injuries(\"https://www.espn.com/nba/injuries\")\n",
    "if df_injury.empty:\n",
    "    print(\"No data or scraping failed.\")\n",
    "\n",
    "# Add a DATA_DATE\n",
    "df_injury[\"DATA_DATE\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = f\"data/injury_reports/injury_report_{df_injury['DATA_DATE'].iloc[0]}.csv\"\n",
    "df_injury.to_csv(csv_file, index=False)\n",
    "print(f\"Saved injury data to {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_injury.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Star Player Identification\n",
    "\n",
    "You can use “typical_starters_dict” or any advanced approach to identify who’s considered a “key player” for each team. Then, if they’re out, your model can see a bigger effect on the minutes or usage of the rest of the team.\n",
    "\n",
    "Minutes vs. Points\n",
    "\n",
    "Often, you’ll first build a minutes model (that uses IS_OUT, TEAM_HAS_STAR_OUT) and outputs MIN_PROJ. Then you feed MIN_PROJ + other features into your points model.\n",
    "The partial historical injuries help that minutes model learn “When star is out, player X’s minutes jump by 5.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pipeline Automation & Data Storage\n",
    "4.1 Scheduled Updates\n",
    "Set up a daily or weekly job that:\n",
    "Pulls fresh game logs and advanced box scores via the NBA API (or your own data store).\n",
    "Updates your training dataset.\n",
    "Optionally retrains or re-fits the model.\n",
    "Generates new next-game predictions for each player.\n",
    "4.2 Data Storage\n",
    "Use a database (e.g., SQLite, PostgreSQL) or a Cloud Data Warehouse (BigQuery, Snowflake) to store:\n",
    "Historical game logs\n",
    "Player metadata (e.g., birthdate, draft year, position, injuries)\n",
    "Team stats / synergy metrics\n",
    "This central repository simplifies repeated queries and ensures you have a single source of truth.\n",
    "4.3 Version Control & Logging\n",
    "Version your model artifacts (e.g., model_v1.0.pkl, model_v1.1.pkl) and keep a record of:\n",
    "Date of training\n",
    "Hyperparameters\n",
    "Performance metrics\n",
    "Log daily predictions and compare them later to actual game results to measure real-time accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Perform a Residual Analysis\n",
    "Generate a residual DataFrame: df_residuals = actual_points - predicted_points.\n",
    "Plot histograms, scatter plots (residuals vs. minutes or usage), or groupby stats (residuals by team or position).\n",
    "Look for patterns that might suggest missing features or systematic biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.cleanup_script import remove_markdown_blocks_and_reformat\n",
    "\n",
    "remove_markdown_blocks_and_reformat(\"notebooks/test.py\", \"notebooks/test_cleaned.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NBA API\n",
    "from nba_api.stats.endpoints import (\n",
    "    playergamelog, boxscoreadvancedv2, leaguedashteamstats, scoreboardv2,\n",
    "    commonplayerinfo, leaguegamefinder, boxscoretraditionalv2,\n",
    "    # *** NEW ***\n",
    "    commonteamroster, playercareerstats\n",
    ")\n",
    "from nba_api.stats.static import players, teams\n",
    "\n",
    "# Scikit-Learn & Modeling\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor)\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 1. Utility & Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_player_id(player_name):\n",
    "    \"\"\"Get the NBA player ID given the player's full name.\"\"\"\n",
    "    nba_players = players.get_players()\n",
    "    player = next((p for p in nba_players if p['full_name'].lower() == player_name.lower()), None)\n",
    "    return player['id'] if player else None\n",
    "\n",
    "\n",
    "def get_team_abbreviation_id_mapping():\n",
    "    \"\"\"Return a dict mapping team abbreviations to team IDs.\"\"\"\n",
    "    nba_teams = teams.get_teams()\n",
    "    return {team['abbreviation']: team['id'] for team in nba_teams}\n",
    "\n",
    "\n",
    "def get_player_team_id(player_id):\n",
    "    \"\"\"Get the player's current team ID.\"\"\"\n",
    "    try:\n",
    "        df = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        return int(df['TEAM_ID'].iloc[0])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_team_name(team_id):\n",
    "    \"\"\"Get the full team name given the team ID.\"\"\"\n",
    "    nba_teams = teams.get_teams()\n",
    "    team = next((t for t in nba_teams if t['id'] == team_id), None)\n",
    "    return team['full_name'] if team else 'Unknown Team'\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 2. Data Fetching Functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_player_game_logs(player_id, season='2024-25'):\n",
    "    \"\"\"Fetch player game logs for the given season.\"\"\"\n",
    "    try:\n",
    "        gamelog = playergamelog.PlayerGameLog(player_id=player_id, season=season, timeout=60)\n",
    "        df = gamelog.get_data_frames()[0]\n",
    "        df.columns = df.columns.str.upper()\n",
    "        return df\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_player_advanced_stats(player_id, season='2024-25'):\n",
    "    \"\"\"Fetch advanced stats for all games played by the player in the specified season.\"\"\"\n",
    "    gamelog_df = get_player_game_logs(player_id, season)\n",
    "\n",
    "    # *** FIX ***: If gamelog_df is empty or missing 'GAME_ID', return an empty DataFrame\n",
    "    if gamelog_df.empty or 'GAME_ID' not in gamelog_df.columns:\n",
    "        print(f\"[FIX] No valid game logs (or missing GAME_ID) for player_id={player_id}, season={season}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    adv_stats = []\n",
    "    for game_id in gamelog_df['GAME_ID']:\n",
    "        try:\n",
    "            boxscore = boxscoreadvancedv2.BoxScoreAdvancedV2(game_id=game_id, timeout=60)\n",
    "            p_stats = boxscore.player_stats.get_data_frame()\n",
    "            p_adv = p_stats[p_stats['PLAYER_ID'] == int(player_id)]\n",
    "            adv_stats.append(p_adv)\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    if adv_stats:\n",
    "        df = pd.concat(adv_stats, ignore_index=True)\n",
    "        return df[['GAME_ID', 'PLAYER_ID', 'USG_PCT', 'PIE', 'TEAM_ID', 'OFF_RATING', 'PACE_PER40']]\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_opponent_stats(season='2024-25'):\n",
    "    \"\"\"Fetch opponent defensive stats for all teams.\"\"\"\n",
    "    df = leaguedashteamstats.LeagueDashTeamStats(\n",
    "        season=season,\n",
    "        measure_type_detailed_defense='Defense',\n",
    "        per_mode_detailed='PerGame',\n",
    "        timeout=60\n",
    "    ).get_data_frames()[0]\n",
    "    return df[['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE']]\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 3. Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "def compute_efficiency(df):\n",
    "    \"\"\"Compute an EFF metric.\"\"\"\n",
    "    df['EFF'] = (\n",
    "        df['PTS'] + df['REB'] + df['AST'] + df['STL'] + df['BLK']\n",
    "        - (df['FGA'] - df['FGM']) - (df['FTA'] - df['FTM']) - df['TOV']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_true_shooting_percentage(df):\n",
    "    df['TS_DENOM'] = 2 * (df['FGA'] + 0.44 * df['FTA'])\n",
    "    df['TS_PCT'] = df.apply(\n",
    "        lambda row: row['PTS'] / row['TS_DENOM'] if row['TS_DENOM'] != 0 else 0, axis=1\n",
    "    )\n",
    "    df.drop(columns=['TS_DENOM'], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_player_position(player_id, cache=None):\n",
    "    \"\"\"\n",
    "    Map raw position strings to G/F/C.\n",
    "    \"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    if player_id in cache:\n",
    "        return cache[player_id]\n",
    "    pos = None\n",
    "    try:\n",
    "        info = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        raw_pos = info.get('POSITION', [''])[0]\n",
    "        if isinstance(raw_pos, str):\n",
    "            main_pos = raw_pos.split('-')[0].title()\n",
    "            if 'Guard' in main_pos:\n",
    "                pos = 'G'\n",
    "            elif 'Forward' in main_pos:\n",
    "                pos = 'F'\n",
    "            elif 'Center' in main_pos:\n",
    "                pos = 'C'\n",
    "    except:\n",
    "        pass\n",
    "    cache[player_id] = pos\n",
    "    return pos\n",
    "\n",
    "\n",
    "def feature_engineering(player_df, adv_df, opp_df, team_map):\n",
    "    # 1) Basic computations\n",
    "    player_df = compute_efficiency(player_df)\n",
    "    player_df = compute_true_shooting_percentage(player_df)\n",
    "\n",
    "    # 2) Merge advanced stats\n",
    "    df = pd.merge(player_df, adv_df, on=['GAME_ID', 'PLAYER_ID'], how='left')\n",
    "    df['OPPONENT_ABBREVIATION'] = df['MATCHUP'].str.split(' ').str[-1]\n",
    "    df['OPPONENT_TEAM_ID'] = df['OPPONENT_ABBREVIATION'].map(team_map)\n",
    "\n",
    "    # 3) Merge defensive stats (if provided)\n",
    "    if opp_df is not None and not opp_df.empty:\n",
    "        df = pd.merge(df, opp_df, left_on='OPPONENT_TEAM_ID', right_on='TEAM_ID', how='left')\n",
    "        for col in ['DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_2ND_CHANCE']:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    # 4) Sort by date\n",
    "    df.sort_values(['PLAYER_NAME', 'GAME_DATE'], inplace=True)\n",
    "    df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'], errors='coerce')\n",
    "\n",
    "    # 5) Convert minutes to float\n",
    "    def parse_minutes(x):\n",
    "        if isinstance(x, str) and ':' in x:\n",
    "            mins, secs = x.split(':')\n",
    "            return float(mins) + float(secs)/60\n",
    "        return float(x) if pd.notna(x) else 0\n",
    "\n",
    "    df['MIN'] = df['MIN'].apply(parse_minutes)\n",
    "    df['FG_PCT'] = df['FGM'] / df['FGA'].replace(0, np.nan)\n",
    "\n",
    "    # 6) Compute rolling stats\n",
    "    rolling_cols = ['PIE', 'USG_PCT', 'PTS', 'REB', 'AST', 'EFF', 'TS_PCT', 'MIN', 'FG_PCT', 'OFF_RATING', 'PACE_PER40']\n",
    "    for c in rolling_cols:\n",
    "        df[f'{c}_AVG_LAST_5'] = df.groupby('PLAYER_NAME')[c].transform(\n",
    "            lambda x: x.shift(1).rolling(window=5, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "    for c in ['PTS', 'USG_PCT', 'MIN']:\n",
    "        df[f'{c}_VOL_LAST_5'] = df.groupby('PLAYER_NAME')[c].transform(\n",
    "            lambda x: x.shift(1).rolling(window=5, min_periods=1).std()\n",
    "        )\n",
    "\n",
    "    # 7) Compute season average for PTS\n",
    "    df['PTS_SEASON_AVG'] = df.groupby('PLAYER_NAME')['PTS'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "\n",
    "    # 8) Additional features\n",
    "    df['HOME_GAME'] = df['MATCHUP'].apply(lambda x: 1 if 'vs.' in x else 0)\n",
    "    df['REST_DAYS'] = df.groupby('PLAYER_NAME')['GAME_DATE'].diff().dt.days.fillna(0)\n",
    "\n",
    "    # Clean up\n",
    "    if 'TEAM_ID_x' in df.columns:\n",
    "        df.rename(columns={'TEAM_ID_x': 'TEAM_ID'}, inplace=True)\n",
    "        df.drop(columns=['TEAM_ID_y'], errors='ignore', inplace=True)\n",
    "\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 4. Aggregator Functions (Position & Team vs Opponent)\n",
    "# =============================================================================\n",
    "\n",
    "def add_player_position(df):\n",
    "    \"\"\"Add a 'POSITION' column (G, F, C) to each row.\"\"\"\n",
    "    cache = {}\n",
    "    df['POSITION'] = df['PLAYER_ID'].apply(lambda pid: get_player_position(pid, cache))\n",
    "    return df.dropna(subset=['POSITION'])\n",
    "\n",
    "\n",
    "def compute_position_allowed_pts(df):\n",
    "    \"\"\"Calculate how many points each team concedes on average to a specific position.\"\"\"\n",
    "    agg = df.groupby(['OPPONENT_TEAM_ID', 'POSITION'])['PTS'].mean().reset_index()\n",
    "    agg.rename(columns={'PTS': 'OPPONENT_POSITION_ALLOWED_PTS'}, inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def add_opponent_position_allowed_pts(df):\n",
    "    \"\"\"Merge the 'OPPONENT_POSITION_ALLOWED_PTS' back to the main DataFrame.\"\"\"\n",
    "    df_pos = add_player_position(df)\n",
    "    agg = compute_position_allowed_pts(df_pos)\n",
    "    return pd.merge(df_pos, agg, on=['OPPONENT_TEAM_ID', 'POSITION'], how='left')\n",
    "\n",
    "\n",
    "def compute_team_vs_opponent_allowed_pts(df):\n",
    "    \"\"\"Compute average points a TEAM_ID scores vs. a specific OPPONENT_TEAM_ID.\"\"\"\n",
    "    agg = df.groupby(['TEAM_ID', 'OPPONENT_TEAM_ID'])['PTS'].mean().reset_index(name='TEAM_VS_OPP_ALLOWED_PTS')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def add_team_vs_opponent_allowed_pts(df):\n",
    "    \"\"\"Merge 'TEAM_VS_OPP_ALLOWED_PTS' back to the main DataFrame.\"\"\"\n",
    "    agg = compute_team_vs_opponent_allowed_pts(df)\n",
    "    return pd.merge(df, agg, on=['TEAM_ID', 'OPPONENT_TEAM_ID'], how='left')\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 5. Data Splitting & Preparation\n",
    "# =============================================================================\n",
    "\n",
    "DEFAULT_FEATURE_COLS = [\n",
    "    'PIE_AVG_LAST_5',\n",
    "    'USG_PCT_AVG_LAST_5',\n",
    "    'EFF_AVG_LAST_5',\n",
    "    'TS_PCT_AVG_LAST_5',\n",
    "    'DEF_RATING',\n",
    "    'OPP_PTS_OFF_TOV',\n",
    "    'OPP_PTS_2ND_CHANCE',\n",
    "    'HOME_GAME',\n",
    "    'REST_DAYS',\n",
    "    'PTS_AVG_LAST_5',\n",
    "    'REB_AVG_LAST_5',\n",
    "    'AST_AVG_LAST_5',\n",
    "    'FG_PCT_AVG_LAST_5',\n",
    "    'MIN_AVG_LAST_5',\n",
    "    'OFF_RATING_AVG_LAST_5',\n",
    "    'PACE_PER40_AVG_LAST_5',\n",
    "    'PTS_SEASON_AVG',\n",
    "    'OPPONENT_POSITION_ALLOWED_PTS',\n",
    "    'TEAM_VS_OPP_ALLOWED_PTS',\n",
    "    'PTS_VOL_LAST_5',\n",
    "    'USG_PCT_VOL_LAST_5',\n",
    "    'MIN_VOL_LAST_5',\n",
    "    # *** NEW *** Add star-missing if you want it in the model\n",
    "    # 'STAR_PLAYERS_MISSING_COUNT'\n",
    "]\n",
    "\n",
    "\n",
    "def prepare_data(df, feature_cols=DEFAULT_FEATURE_COLS):\n",
    "    \"\"\"\n",
    "    Where KeyErrors can happen if 'df' lacks columns in 'feature_cols'.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=feature_cols)\n",
    "    X = df[feature_cols].copy()\n",
    "    X['PLAYER_NAME'] = df['PLAYER_NAME']\n",
    "    y = df['PTS']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    X_test_original = X_test.copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.drop(columns=['PLAYER_NAME']))\n",
    "    X_test_scaled = scaler.transform(X_test.drop(columns=['PLAYER_NAME']))\n",
    "\n",
    "    os.makedirs('lib', exist_ok=True)\n",
    "    joblib.dump(scaler, 'lib/scaler.pkl')\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X_test_original\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 6. Modeling & Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'CatBoost': CatBoostRegressor(random_state=42, verbose=0),\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ridge': Ridge(),\n",
    "        'BayesianRidge': BayesianRidge(),\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        print(f\"\\n{name} Performance:\\n  RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "\n",
    "    print(f\"\\nBest model: {type(best_model).__name__} with RMSE: {best_rmse:.2f}\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test_scaled, y_test, X_test_original):\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"\\nEvaluation on Test Data:\\n  RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "    eval_df = X_test_original.reset_index(drop=True).copy()\n",
    "    eval_df['Actual_PTS'] = y_test.reset_index(drop=True)\n",
    "    eval_df['Predicted_PTS'] = preds\n",
    "    eval_df['Residual'] = eval_df['Actual_PTS'] - eval_df['Predicted_PTS']\n",
    "\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(eval_df[['PLAYER_NAME', 'Actual_PTS', 'Predicted_PTS', 'Residual']].head(10))\n",
    "    return eval_df\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 7. Next-Game Prediction Logic\n",
    "# =============================================================================\n",
    "\n",
    "def get_team_defensive_stats(team_id, season='2024-25'):\n",
    "    try:\n",
    "        df = leaguedashteamstats.LeagueDashTeamStats(\n",
    "            team_id_nullable=team_id,\n",
    "            season=season,\n",
    "            measure_type_detailed_defense='Defense',\n",
    "            per_mode_detailed='PerGame',\n",
    "            timeout=60\n",
    "        ).get_data_frames()[0]\n",
    "        return df[['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_OFF_TOV']].iloc[0]\n",
    "    except:\n",
    "        return pd.Series([np.nan]*4, index=['TEAM_ID', 'DEF_RATING', 'OPP_PTS_OFF_TOV', 'OPP_PTS_OFF_TOV'])\n",
    "\n",
    "\n",
    "def get_next_game_info(player_team_id):\n",
    "    next_game_date = datetime.now()\n",
    "    max_days_ahead = 14\n",
    "\n",
    "    for _ in range(max_days_ahead):\n",
    "        game_date_str = next_game_date.strftime('%Y-%m-%d')\n",
    "        try:\n",
    "            scoreboard = scoreboardv2.ScoreboardV2(game_date=game_date_str)\n",
    "            games = scoreboard.game_header.get_data_frame()\n",
    "            team_games = games[\n",
    "                (games['HOME_TEAM_ID'] == player_team_id) or (games['VISITOR_TEAM_ID'] == player_team_id)\n",
    "            ]\n",
    "            if not team_games.empty:\n",
    "                next_game = team_games.iloc[0]\n",
    "                opponent_team_id = (\n",
    "                    next_game['VISITOR_TEAM_ID'] if next_game['HOME_TEAM_ID'] == player_team_id\n",
    "                    else next_game['HOME_TEAM_ID']\n",
    "                )\n",
    "                home_game = 1 if next_game['HOME_TEAM_ID'] == player_team_id else 0\n",
    "                return next_game_date, opponent_team_id, home_game\n",
    "        except:\n",
    "            pass\n",
    "        next_game_date += timedelta(days=1)\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def prepare_features_for_prediction(player_id, player_name, season='2024-25',\n",
    "                                    feature_cols=DEFAULT_FEATURE_COLS,\n",
    "                                    df_agg_position=None, df_agg_team_vs_opp=None):\n",
    "\n",
    "    logs_df = get_player_game_logs(player_id, season)\n",
    "    if logs_df.empty:\n",
    "        print(f\"No game logs for {player_name} in {season}.\")\n",
    "        return None, None\n",
    "    logs_df['GAME_DATE'] = pd.to_datetime(logs_df['GAME_DATE'])\n",
    "    logs_df.sort_values('GAME_DATE', ascending=False, inplace=True)\n",
    "    logs_df['PLAYER_NAME'] = player_name\n",
    "\n",
    "    p_team_id = get_player_team_id(player_id)\n",
    "    next_game_date, opp_team_id, home_game = get_next_game_info(p_team_id)\n",
    "    if not next_game_date:\n",
    "        print(f\"No upcoming game found for {player_name}.\")\n",
    "        return None, None\n",
    "\n",
    "    opp_stats = get_team_defensive_stats(opp_team_id, season)\n",
    "    adv_df = get_player_advanced_stats(player_id, season)\n",
    "\n",
    "    # We fetch the opponent stats so columns exist\n",
    "    season_opp_df = get_opponent_stats(season=season)\n",
    "    team_map = get_team_abbreviation_id_mapping()\n",
    "\n",
    "    recent_logs = logs_df[logs_df['GAME_DATE'] <= logs_df['GAME_DATE'].max()]\n",
    "\n",
    "    # If adv_df is empty or no GAME_ID, the next steps won't break\n",
    "    if adv_df.empty or 'GAME_ID' not in adv_df.columns:\n",
    "        print(f\"[FIX] adv_df is empty or missing 'GAME_ID' for {player_name}, continuing with partial data...\")\n",
    "        # We'll create a minimal adv_df with same columns so merges won't fail:\n",
    "        adv_df = pd.DataFrame(columns=['GAME_ID','PLAYER_ID','USG_PCT','PIE','TEAM_ID','OFF_RATING','PACE_PER40'])\n",
    "\n",
    "    adv_df = adv_df[adv_df['GAME_ID'].isin(recent_logs['GAME_ID'])]\n",
    "    final_df = feature_engineering(recent_logs, adv_df, season_opp_df, team_map)\n",
    "    if final_df.empty:\n",
    "        return None, None\n",
    "\n",
    "    latest_data = final_df.iloc[-1].copy()\n",
    "    latest_data['REST_DAYS'] = (next_game_date - latest_data['GAME_DATE']).days\n",
    "    latest_data['HOME_GAME'] = home_game\n",
    "    latest_data['GAME_DATE'] = next_game_date\n",
    "    latest_data['OPPONENT_TEAM_ID'] = opp_team_id\n",
    "    latest_data['DEF_RATING'] = opp_stats['DEF_RATING']\n",
    "    latest_data['OPP_PTS_OFF_TOV'] = opp_stats['OPP_PTS_OFF_TOV']\n",
    "    latest_data['OPP_PTS_2ND_CHANCE'] = opp_stats['OPP_PTS_OFF_TOV']  # possible fix if we want a 4th col\n",
    "    # ^ If you have a mismatch, fix as you see fit.\n",
    "\n",
    "    if df_agg_position is not None:\n",
    "        pos = get_player_position(player_id)\n",
    "        if not pos:\n",
    "            pos = 'G'\n",
    "        row_match = df_agg_position[\n",
    "            (df_agg_position['OPPONENT_TEAM_ID'] == opp_team_id) & (df_agg_position['POSITION'] == pos)\n",
    "        ]\n",
    "        if not row_match.empty:\n",
    "            latest_data['OPPONENT_POSITION_ALLOWED_PTS'] = row_match['OPPONENT_POSITION_ALLOWED_PTS'].iloc[0]\n",
    "        else:\n",
    "            fallback_pos = df_agg_position[df_agg_position['POSITION'] == pos]\n",
    "            latest_data['OPPONENT_POSITION_ALLOWED_PTS'] = fallback_pos['OPPONENT_POSITION_ALLOWED_PTS'].mean()\n",
    "\n",
    "    if df_agg_team_vs_opp is not None:\n",
    "        row_match = df_agg_team_vs_opp[\n",
    "            (df_agg_team_vs_opp['TEAM_ID'] == p_team_id) & (df_agg_team_vs_opp['OPPONENT_TEAM_ID'] == opp_team_id)\n",
    "        ]\n",
    "        if not row_match.empty:\n",
    "            latest_data['TEAM_VS_OPP_ALLOWED_PTS'] = row_match['TEAM_VS_OPP_ALLOWED_PTS'].iloc[0]\n",
    "        else:\n",
    "            latest_data['TEAM_VS_OPP_ALLOWED_PTS'] = df_agg_team_vs_opp['TEAM_VS_OPP_ALLOWED_PTS'].mean()\n",
    "\n",
    "    try:\n",
    "        fv = latest_data[feature_cols].values.reshape(1, -1)\n",
    "        return fv, latest_data\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing columns for {player_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def predict_upcoming_points(player_name, season='2024-25',\n",
    "                            feature_cols=DEFAULT_FEATURE_COLS,\n",
    "                            df_agg_position=None, df_agg_team_vs_opp=None):\n",
    "    player_id = get_player_id(player_name)\n",
    "    if not player_id:\n",
    "        print(f\"Invalid player: {player_name}\")\n",
    "        return None\n",
    "\n",
    "    fv, latest_data = prepare_features_for_prediction(\n",
    "        player_id, player_name, season,\n",
    "        feature_cols=feature_cols,\n",
    "        df_agg_position=df_agg_position,\n",
    "        df_agg_team_vs_opp=df_agg_team_vs_opp\n",
    "    )\n",
    "    if fv is None:\n",
    "        return None\n",
    "\n",
    "    scaler = joblib.load('lib/scaler.pkl')\n",
    "    model = joblib.load('lib/player_points_model.pkl')\n",
    "    fv_scaled = scaler.transform(fv)\n",
    "    pred = model.predict(fv_scaled)\n",
    "\n",
    "    opp_name = get_team_name(latest_data['OPPONENT_TEAM_ID'])\n",
    "    date_str = latest_data['GAME_DATE'].strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Predicted points for {player_name} on {date_str} vs {opp_name}: {pred[0]:.2f}\")\n",
    "    return pred[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 8. Scrape ESPN Injury Data\n",
    "# =============================================================================\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_espn_injuries(url=\"https://www.espn.com/nba/injuries\"):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "    [TEAM_NAME, PLAYER_NAME, POS, EST_RETURN, STATUS, COMMENT, DATA_DATE, IS_OUT, IS_DAY_TO_DAY, IS_QUESTIONABLE].\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/107.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    team_tables = soup.find_all(\"div\", class_=\"ResponsiveTable Table__league-injuries\")\n",
    "\n",
    "    all_rows = []\n",
    "    for table_div in team_tables:\n",
    "        title_div = table_div.find(\"div\", class_=\"Table__Title\")\n",
    "        if not title_div:\n",
    "            continue\n",
    "        team_name_span = title_div.find(\"span\", class_=\"injuries__teamName\")\n",
    "        if not team_name_span:\n",
    "            continue\n",
    "        team_name = team_name_span.get_text(strip=True)\n",
    "\n",
    "        table_tag = table_div.find(\"table\", class_=\"Table\")\n",
    "        if not table_tag:\n",
    "            continue\n",
    "\n",
    "        tbody = table_tag.find(\"tbody\", class_=\"Table__TBODY\")\n",
    "        if not tbody:\n",
    "            continue\n",
    "\n",
    "        rows = tbody.find_all(\"tr\", class_=\"Table__TR\")\n",
    "        for row in rows:\n",
    "            tds = row.find_all(\"td\", class_=\"Table__TD\")\n",
    "            if len(tds) < 5:\n",
    "                continue\n",
    "\n",
    "            player_name = tds[0].get_text(strip=True)\n",
    "            pos = tds[1].get_text(strip=True)\n",
    "            est_return = tds[2].get_text(strip=True)\n",
    "            status = tds[3].get_text(strip=True)\n",
    "            comment = tds[4].get_text(strip=True)\n",
    "\n",
    "            is_out = 1 if 'out' in status.lower() else 0\n",
    "            is_questionable = 1 if 'questionable' in status.lower() else 0\n",
    "            is_day_to_day = 1 if 'day-to-day' in status.lower() else 0\n",
    "\n",
    "            all_rows.append({\n",
    "                \"TEAM_NAME\": team_name,\n",
    "                \"PLAYER_NAME\": player_name,\n",
    "                \"POS\": pos,\n",
    "                \"EST_RETURN\": est_return,\n",
    "                \"STATUS\": status,\n",
    "                \"COMMENT\": comment,\n",
    "                \"DATA_DATE\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                \"IS_OUT\": is_out,\n",
    "                \"IS_QUESTIONABLE\": is_questionable,\n",
    "                \"IS_DAY_TO_DAY\": is_day_to_day\n",
    "            })\n",
    "\n",
    "    df_injury = pd.DataFrame(all_rows)\n",
    "    return df_injury\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 9. Star Player & Missing-Star Logic\n",
    "# =============================================================================\n",
    "\n",
    "def get_team_roster(team_id, season='2023-24'):\n",
    "    \"\"\"Use nba_api's commonteamroster to get the team's current season roster.\"\"\"\n",
    "    try:\n",
    "        roster = commonteamroster.CommonTeamRoster(team_id=team_id, season=season).get_data_frames()[0]\n",
    "        return roster\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_player_ppg_prior_season(player_id, prior_season='2022-23'):\n",
    "    \"\"\"Return prior-season PPG from nba_api.\"\"\"\n",
    "    try:\n",
    "        career = playercareerstats.PlayerCareerStats(player_id=player_id, timeout=60).get_data_frames()[0]\n",
    "        mask = career['SEASON_ID'].str.contains(prior_season[-4:], na=False)\n",
    "        season_row = career[mask]\n",
    "        if not season_row.empty:\n",
    "            pts = season_row['PTS'].values[0]\n",
    "            gp = season_row['GP'].values[0]\n",
    "            if gp > 0:\n",
    "                return pts / gp\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_star_players_by_ppg(team_id, season='2023-24', prior_season='2022-23', ppg_threshold=20.0):\n",
    "    roster_df = get_team_roster(team_id, season=season)\n",
    "    if roster_df.empty:\n",
    "        return []\n",
    "\n",
    "    star_players = []\n",
    "    for _, row in roster_df.iterrows():\n",
    "        pid = row['PLAYER_ID']\n",
    "        name = row['PLAYER']\n",
    "        prior_ppg = get_player_ppg_prior_season(pid, prior_season)\n",
    "        if prior_ppg >= ppg_threshold:\n",
    "            star_players.append((pid, name, prior_ppg))\n",
    "    return star_players\n",
    "\n",
    "\n",
    "def parse_minutes_to_float(min_str):\n",
    "    if isinstance(min_str, str) and ':' in min_str:\n",
    "        mm, ss = min_str.split(':')\n",
    "        return float(mm) + float(ss)/60.0\n",
    "    try:\n",
    "        return float(min_str)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_boxscore_participants(game_id):\n",
    "    \"\"\"\n",
    "    For past/historical games, check who actually played (MIN>0).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        box_df = boxscoretraditionalv2.BoxScoreTraditionalV2(game_id=game_id, timeout=60).get_data_frames()[0]\n",
    "        box_df['MIN_float'] = box_df['MIN'].apply(parse_minutes_to_float)\n",
    "        played = box_df[box_df['MIN_float'] > 0]['PLAYER_ID'].unique()\n",
    "        return set(played)\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def is_star_player_out_future(df_injury, team_name, player_name):\n",
    "    \"\"\"\n",
    "    If star player is in df_injury with IS_OUT=1, IS_QUESTIONABLE=1, or IS_DAY_TO_DAY=1 => treat as missing.\n",
    "    \"\"\"\n",
    "    if df_injury.empty:\n",
    "        return False\n",
    "    cond_team = df_injury['TEAM_NAME'].str.lower() == team_name.lower()\n",
    "    cond_player = df_injury['PLAYER_NAME'].str.lower() == player_name.lower()\n",
    "    sub = df_injury[cond_team & cond_player]\n",
    "    if sub.empty:\n",
    "        return False\n",
    "\n",
    "    row = sub.iloc[0]\n",
    "    if row['IS_OUT'] == 1:\n",
    "        return True\n",
    "    if row['IS_QUESTIONABLE'] == 1:\n",
    "        return True\n",
    "    if row['IS_DAY_TO_DAY'] == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_missing_star_players_for_game(game_id, game_date, team_id, star_players_list, df_injury, team_id_to_name):\n",
    "    missing = []\n",
    "    now_date = datetime.now().date()\n",
    "    if isinstance(game_date, pd.Timestamp):\n",
    "        game_date = game_date.date()\n",
    "\n",
    "    if game_date < now_date:\n",
    "        # Past game => who actually played\n",
    "        participants = get_boxscore_participants(game_id)\n",
    "        for (pid, pname, ppg) in star_players_list:\n",
    "            if pid not in participants:\n",
    "                missing.append((pid, pname, ppg))\n",
    "    else:\n",
    "        # Future => check injury\n",
    "        team_name = team_id_to_name.get(team_id, 'Unknown Team')\n",
    "        for (pid, pname, ppg) in star_players_list:\n",
    "            if is_star_player_out_future(df_injury, team_name, pname):\n",
    "                missing.append((pid, pname, ppg))\n",
    "\n",
    "    return missing\n",
    "\n",
    "\n",
    "def add_missing_star_player_feature(player_df, p_team_id, star_players_list, df_injury, team_id_to_name):\n",
    "    \"\"\"\n",
    "    For each row (game), set STAR_PLAYERS_MISSING_COUNT = # star teammates who didn't play.\n",
    "    \"\"\"\n",
    "    if 'GAME_ID' not in player_df.columns or 'GAME_DATE' not in player_df.columns:\n",
    "        return player_df\n",
    "\n",
    "    player_df['STAR_PLAYERS_MISSING_COUNT'] = 0\n",
    "    for idx, row in player_df.iterrows():\n",
    "        game_id = row['GAME_ID']\n",
    "        game_date = row['GAME_DATE']\n",
    "        missing_list = get_missing_star_players_for_game(\n",
    "            game_id=game_id,\n",
    "            game_date=game_date,\n",
    "            team_id=p_team_id,\n",
    "            star_players_list=star_players_list,\n",
    "            df_injury=df_injury,\n",
    "            team_id_to_name=team_id_to_name\n",
    "        )\n",
    "        player_df.at[idx, 'STAR_PLAYERS_MISSING_COUNT'] = len(missing_list)\n",
    "\n",
    "    return player_df\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 10. Example Usage\n",
    "# =============================================================================\n",
    "\n",
    "def build_dataset_with_missing_stars(\n",
    "    player_names,\n",
    "    df_injury,\n",
    "    season='2024-25',\n",
    "    prior_season='2023-24'\n",
    "):\n",
    "    all_data = pd.DataFrame()\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_id_to_name = {t['id']: t['full_name'] for t in nba_teams}\n",
    "\n",
    "    opp_df = get_opponent_stats(season)  # ensures DEF_RATING etc. come from real data\n",
    "    team_map = get_team_abbreviation_id_mapping()\n",
    "\n",
    "    for p_name in player_names:\n",
    "        p_id = get_player_id(p_name)\n",
    "        if not p_id:\n",
    "            continue\n",
    "        logs_df = get_player_game_logs(p_id, season=season)\n",
    "        if logs_df.empty:\n",
    "            print(f\"[build_dataset_with_missing_stars] No logs for {p_name}, skip.\")\n",
    "            continue\n",
    "        logs_df['PLAYER_NAME'] = p_name\n",
    "        logs_df['GAME_DATE'] = pd.to_datetime(logs_df['GAME_DATE'])\n",
    "\n",
    "        p_team_id = get_player_team_id(p_id)\n",
    "        if not p_team_id:\n",
    "            continue\n",
    "\n",
    "        star_players_list = get_star_players_by_ppg(\n",
    "            team_id=p_team_id,\n",
    "            season=season,\n",
    "            prior_season=prior_season,\n",
    "            ppg_threshold=20.0\n",
    "        )\n",
    "\n",
    "        logs_df = add_missing_star_player_feature(\n",
    "            player_df=logs_df,\n",
    "            p_team_id=p_team_id,\n",
    "            star_players_list=star_players_list,\n",
    "            df_injury=df_injury,\n",
    "            team_id_to_name=team_id_to_name\n",
    "        )\n",
    "\n",
    "        adv_df = get_player_advanced_stats(p_id, season)\n",
    "        merged_df = feature_engineering(logs_df, adv_df, opp_df, team_map)\n",
    "\n",
    "        all_data = pd.concat([all_data, merged_df], ignore_index=True)\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_injury = scrape_espn_injuries(\"https://www.espn.com/nba/injuries\")\n",
    "player_names = [\n",
    "    \"LeBron James\", \"Stephen Curry\", \"Trae Young\", \"Jalen Brunson\"]\n",
    "    \n",
    "player_names = [\n",
    "    \"Kevin Durant\", \"Stephen Curry\",\n",
    "    \"Giannis Antetokounmpo\", \"Luka Dončić\", \"Joel Embiid\",\n",
    "    \"Jayson Tatum\", \"Nikola Jokić\", \"Shai Gilgeous-Alexander\",\n",
    "    \"Karl-Anthony Towns\", \"Victor Wembanyama\", \"Damian Lillard\",\n",
    "    \"Donovan Mitchell\", \"Anthony Davis\", \"Domantas Sabonis\",\n",
    "    \"James Harden\", \"Kyrie Irving\", \"Anthony Edwards\", \"Jimmy Butler\",\n",
    "    \"De'Aaron Fox\", \"Jalen Brunson\", \"Bronny James\", \"Tyrese Maxey\", \n",
    "    \"Trae Young\", \"Pascal Siakam\"]\n",
    "\n",
    "season = '2024-25'\n",
    "prior_season = '2023-24'\n",
    "\n",
    "# *** Build the dataset with defensive stats + star-absences\n",
    "all_player_data = build_dataset_with_missing_stars(player_names, df_injury, season=season, prior_season=prior_season)\n",
    "\n",
    "# *** Then aggregator columns:\n",
    "all_player_data = add_opponent_position_allowed_pts(all_player_data)\n",
    "all_player_data = add_team_vs_opponent_allowed_pts(all_player_data)\n",
    "\n",
    "# *** NEW *** If you want the star-absences in your model features:\n",
    "if 'STAR_PLAYERS_MISSING_COUNT' not in DEFAULT_FEATURE_COLS:\n",
    "    DEFAULT_FEATURE_COLS.append('STAR_PLAYERS_MISSING_COUNT')\n",
    "\n",
    "# Now we can prepare data, ensuring 'DEF_RATING' etc. exist\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, X_test_original = prepare_data(all_player_data, feature_cols=DEFAULT_FEATURE_COLS)\n",
    "best_model = train_and_evaluate_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "joblib.dump(best_model, 'lib/player_points_model.pkl')\n",
    "\n",
    "eval_df = evaluate_model(best_model, X_test_scaled, y_test, X_test_original)\n",
    "\n",
    "# For next-game predictions:\n",
    "df_agg_position = compute_position_allowed_pts(all_player_data)\n",
    "df_agg_team_opp = compute_team_vs_opponent_allowed_pts(all_player_data)\n",
    "\n",
    "for name in player_names:\n",
    "    predict_upcoming_points(\n",
    "            name, season=season,\n",
    "            df_agg_position=df_agg_position,\n",
    "            df_agg_team_vs_opp=df_agg_team_opp\n",
    "        )\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(eval_df['Residual'], kde=True, bins=20)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, X_test_scaled, y_test, X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
